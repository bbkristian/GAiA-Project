{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code demonstrates how to preprocess a dataset, create dummy variables for categorical features, \n",
    "and train a neural network model using TensorFlow to predict energy consumption per hour.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "\"\"\"\n",
    "# Loading data\n",
    "data = pd.read_csv(\"finaldata.csv\")  # Read the data from a CSV file\n",
    "\n",
    "# Convert the 'DATE' column to datetime format\n",
    "data[\"DATE\"] = pd.to_datetime(data[\"DATE\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Set the 'DATE' column as index and reset the index\n",
    "data.set_index(\"DATE\", inplace=True)\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Create new columns for date, id, hour, day_of_week, and energy_per_hour\n",
    "data[\"date\"] = data[\"DATE\"]\n",
    "data[\"id\"] = data[\"LCLid\"]\n",
    "data['hour'] = data[\"HOUR\"]\n",
    "data['day_of_week'] = data[\"DATE\"].dt.day_of_week\n",
    "data['energy_per_hour'] = data[\"ENERGY.PER.HOUR\"]\n",
    "\n",
    "# Create dummy variables for 'day_of_week' and concatenate with the original DataFrame\n",
    "dummies = pd.get_dummies(data['day_of_week'], prefix='day_of_week')\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "\n",
    "# Create dummy variables for 'hour' and concatenate with the original DataFrame\n",
    "dummies = pd.get_dummies(data['hour'], prefix='hour')\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "\n",
    "# Filter the data to only include records with id 'MAC000002'\n",
    "data['year'] = data['date'].dt.year\n",
    "data = data.query(\"id == 'MAC000002'\")\n",
    "\n",
    "# Reset the index and sort the data by date and hour\n",
    "data.reset_index(inplace=True)\n",
    "data = data.drop(columns=[\"index\"])\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "data = data.sort_values(by=[\"date\", \"hour\"]).reset_index(drop=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=[\"DATE\", \"LCLid\", \"HOUR\", \"ENERGY.PER.HOUR\", \"id\", \"day_of_week\", \"hour\"])\n",
    "data = data.drop(columns=[\"X\", \"MINUTES\", \"SECONDS\", \"precipType\", \"icon\", \"summary\", \"Acorn\", \"year\"])\n",
    "data = data.drop(columns=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input (X) and output (y) data for the model\n",
    "X = data.drop(columns=['energy_per_hour'])\n",
    "y = data['energy_per_hour'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.1677 - val_loss: 0.1831\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1503 - val_loss: 0.1812\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1464 - val_loss: 0.1807\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1435 - val_loss: 0.1806\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1419 - val_loss: 0.1834\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1402 - val_loss: 0.1805\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1397 - val_loss: 0.1800\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1380 - val_loss: 0.1821\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1373 - val_loss: 0.1819\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1368 - val_loss: 0.1821\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1362 - val_loss: 0.1832\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1347 - val_loss: 0.1821\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1826\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1336 - val_loss: 0.1818\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1331 - val_loss: 0.1828\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1324 - val_loss: 0.1837\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1318 - val_loss: 0.1817\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1316 - val_loss: 0.1808\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1297 - val_loss: 0.1836\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1301 - val_loss: 0.1819\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1289 - val_loss: 0.1810\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1277 - val_loss: 0.1812\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1286 - val_loss: 0.1827\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1278 - val_loss: 0.1828\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1269 - val_loss: 0.1842\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1261 - val_loss: 0.1866\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1247 - val_loss: 0.1893\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1260 - val_loss: 0.1832\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1243 - val_loss: 0.1846\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1240 - val_loss: 0.1857\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.1875\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1228 - val_loss: 0.1862\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1220 - val_loss: 0.1898\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1214 - val_loss: 0.1884\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1213 - val_loss: 0.1903\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1212 - val_loss: 0.1869\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1203 - val_loss: 0.1901\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1205 - val_loss: 0.1922\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1215 - val_loss: 0.1902\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1190 - val_loss: 0.1890\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.1905\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1184 - val_loss: 0.1921\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1179 - val_loss: 0.1980\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1164 - val_loss: 0.1906\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1167 - val_loss: 0.1913\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1168 - val_loss: 0.1924\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1157 - val_loss: 0.1917\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1143 - val_loss: 0.1927\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1142 - val_loss: 0.1945\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1142 - val_loss: 0.1969\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1128 - val_loss: 0.1974\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1127 - val_loss: 0.1968\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1117 - val_loss: 0.1942\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1121 - val_loss: 0.1983\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1112 - val_loss: 0.2006\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1103 - val_loss: 0.1997\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 0.2056\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1105 - val_loss: 0.2005\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1103 - val_loss: 0.1962\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.2030\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1075 - val_loss: 0.2023\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1081 - val_loss: 0.1996\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1072 - val_loss: 0.2019\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1071 - val_loss: 0.2026\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1076 - val_loss: 0.2049\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1057 - val_loss: 0.2041\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1054 - val_loss: 0.2155\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1058 - val_loss: 0.2046\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1042 - val_loss: 0.2167\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1055 - val_loss: 0.2093\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1036 - val_loss: 0.2050\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1030 - val_loss: 0.2104\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1036 - val_loss: 0.2106\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1028 - val_loss: 0.2052\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1022 - val_loss: 0.2065\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1016 - val_loss: 0.2114\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1024 - val_loss: 0.2080\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1012 - val_loss: 0.2058\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1010 - val_loss: 0.2075\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1005 - val_loss: 0.2181\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0996 - val_loss: 0.2089\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1003 - val_loss: 0.2133\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.1000 - val_loss: 0.2074\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0990 - val_loss: 0.2067\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0985 - val_loss: 0.2213\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0980 - val_loss: 0.2109\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0975 - val_loss: 0.2175\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0973 - val_loss: 0.2168\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0970 - val_loss: 0.2134\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.2137\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0960 - val_loss: 0.2152\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0958 - val_loss: 0.2191\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0949 - val_loss: 0.2178\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0946 - val_loss: 0.2117\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0945 - val_loss: 0.2199\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0944 - val_loss: 0.2183\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0935 - val_loss: 0.2133\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.2130\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.2166\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.2161\n",
      "76/76 [==============================] - 0s 693us/step\n",
      "Neural Network RMSE: 0.44650398859578705\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale input data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the three layer neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "\n",
    "print(\"Neural Network RMSE:\", rmse_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to an HDF5 file\n",
    "model.save('production_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
